import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
max_features = 10000  ;maxlen = 200  ;embedding_dim = 50
batch_size = 128 ; epochs = 5
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)
x_train = pad_sequences(x_train, maxlen=maxlen)   
x_test  = pad_sequences(x_test,  maxlen=maxlen)
model = Sequential([
    Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=maxlen),
    LSTM(64, dropout=0.2, recurrent_dropout=0.2),
    Dense(1, activation='sigmoid')])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=1)
loss, acc = model.evaluate(x_test, y_test, verbose=2)
print(f'Test accuracy: {acc:.4f}')
word_index = imdb.get_word_index()
def predict_sentiment(text):
    words = text.lower().split()
    seq = [word_index.get(w, 0) for w in words]                
    seq = [i if i < max_features else 0 for i in seq]          
    padded = pad_sequences([seq], maxlen=maxlen)
    p = model.predict(padded, verbose=0)[0][0]
    return p 
print("Positive example probability:", predict_sentiment("I loved this movie, it was fantastic and moving."))
print("Negative example probability:", predict_sentiment("Terrible film. I hated it and it was boring."))
plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='val')
plt.title('Accuracy'); plt.xlabel('Epoch'); plt.legend(); plt.grid(True)
plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='val')
plt.title('Loss'); plt.xlabel('Epoch'); plt.legend(); plt.grid(True)
plt.tight_layout(); plt.show()
